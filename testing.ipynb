{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "937400a0",
   "metadata": {},
   "source": [
    "# Case Studies in Machine Learning Final Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff03aac",
   "metadata": {},
   "source": [
    "# Import Data from data.austintexas.gov\n",
    "### AAC Outcomes: https://data.austintexas.gov/Health-and-Community-Services/Austin-Animal-Center-Outcomes/9t4d-g238/about_data\n",
    "### AAC Intakes: https://data.austintexas.gov/Health-and-Community-Services/Austin-Animal-Center-Intakes/wter-evkm/about_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3da0d",
   "metadata": {},
   "source": [
    "# Load the data and merge tables then remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b69eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a date for the dataset\n",
    "download_date = date(2024, 11, 5)\n",
    "\n",
    "# format download_date into a string with YYYYMMDD format\n",
    "download_datestr = download_date.strftime('%Y%m%d')\n",
    "\n",
    "def load_animal_data(tabletype='processed', download_datestr=download_datestr):\n",
    "    if tabletype == 'raw':\n",
    "        # Insert date string in YYYYMMDD format into the filename\n",
    "        outcomes_filename = os.path.join('data', f'Austin_Animal_Center_Outcomes_{download_datestr}.csv')\n",
    "        intakes_filename = os.path.join('data', f'Austin_Animal_Center_Intakes_{download_datestr}.csv')\n",
    "\n",
    "        df_outcomes = pd.read_csv(outcomes_filename)\n",
    "        df_intakes = pd.read_csv(intakes_filename)\n",
    "\n",
    "        # Join dataframes on Animal ID\n",
    "        pd.options.display.max_columns = 50\n",
    "        df = pd.merge(df_intakes,df_outcomes,on=[\"Animal ID\"],suffixes=('_intake','_outcome'))\n",
    "\n",
    "        # Drop duplicate columns\n",
    "        cols_intakes = df_intakes.columns\n",
    "        cols_outcomes = df_outcomes.columns\n",
    "\n",
    "        duplicate_prefixes = [\"Name\", \"Animal Type\", \"Breed\", \"Color\"]\n",
    "\n",
    "        for pref in duplicate_prefixes:\n",
    "            if (df[pref + \"_intake\"].dropna() == df[pref + \"_outcome\"].dropna()).all():\n",
    "                df[pref] = df[pref + \"_intake\"]\n",
    "                df = df.drop(columns=[pref + \"_intake\", pref + \"_outcome\"])\n",
    "        \n",
    "        df.to_pickle(os.path.join('data', f'Austin_Animal_Center_Joined_{download_datestr}.pkl'))\n",
    "    elif tabletype == 'joined':\n",
    "        filename = os.path.join('data', f'Austin_Animal_Center_Joined_{download_datestr}.pkl')\n",
    "        df = pd.read_pickle(filename)\n",
    "    elif tabletype == 'processed':\n",
    "        filename = os.path.join('data', f'Austin_Animal_Center_Joined_{download_datestr}_processed.pkl')\n",
    "        df = pd.read_pickle(filename)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_joined = load_animal_data(tabletype='raw', download_datestr=download_datestr)\n",
    "display(df_joined.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181769d7",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50151ffb",
   "metadata": {},
   "source": [
    "### Duration of Stay in Shelter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d43da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the duration between intake and outcome\n",
    "df_joined[\"DateTime_outcome\"] = pd.to_datetime(df_joined[\"DateTime_outcome\"], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "df_joined[\"DateTime_intake\"] = pd.to_datetime(df_joined[\"DateTime_intake\"], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "df_joined[\"duration_in_shelter\"] = df_joined[\"DateTime_outcome\"] - df_joined[\"DateTime_intake\"]\n",
    "df_joined['intake_month'] = df_joined['DateTime_intake'].dt.month\n",
    "df_joined['intake_year'] = df_joined['DateTime_intake'].dt.year\n",
    "\n",
    "# Drop nans in rows that have invalid meanings, Only include male/female sex\n",
    "df_joined = df_joined.dropna(axis=0,subset=[\"Sex upon Outcome\",\"Sex upon Intake\",\"Outcome Type\",\"Age upon Outcome\"])\n",
    "df_joined = df_joined.loc[df_joined[\"Sex upon Intake\"].str.contains(\"Male|Female\")]\n",
    "\n",
    "# Intake condition of Other as Unknown\n",
    "df_joined[\"Intake Condition\"] = df_joined[\"Intake Condition\"].replace(\"Other\", \"Unknown\")\n",
    "\n",
    "# Convert Age upon intake from string to numeric, parse the strings of the form X (years/months) \n",
    "age_strings = df_joined['Age upon Intake'].str.split(' ', expand=True)\n",
    "age_strings.columns = ['age', 'unit']\n",
    "age_strings['age'] = pd.to_numeric(age_strings['age'])\n",
    "age_strings['unit'] = age_strings['unit'].str.replace('s', '')\n",
    "age_strings.loc[age_strings['unit'] == 'month', 'age'] /= 12\n",
    "age_strings.loc[age_strings['unit'] == 'week', 'age'] /= 52\n",
    "age_strings.loc[age_strings['unit'] == 'day', 'age'] /= 365\n",
    "df_joined['age_upon_intake_years'] = age_strings['age']\n",
    "\n",
    "# Convert NaN Names to Unknown name or stay as nan\n",
    "df_joined['Name'] = df_joined['Name'].fillna('Unknown')\n",
    "\n",
    "# Create binary indicator flags for fixed/intact, male/female\n",
    "df_joined['fixed Intake'] = df_joined['Sex upon Intake'].str.contains('Neutered|Spayed')\n",
    "df_joined['fixed Outcome'] = df_joined['Sex upon Outcome'].str.contains('Neutered|Spayed')\n",
    "df_joined['fixed'] = df_joined['fixed Intake'] | df_joined['fixed Outcome']\n",
    "df_joined = df_joined.drop(['fixed Intake', 'fixed Outcome'], axis=1)\n",
    "\n",
    "# 1 means female, 0 means male\n",
    "df_joined['sex'] = df_joined['Sex upon Intake'].str.contains('Female')\n",
    "\n",
    "# Figure out how many durations are negative and drop from dataframe, unfortunately this is a data quality issue\n",
    "df_joined = df_joined.drop(df_joined.loc[df_joined['duration_in_shelter'] < pd.Timedelta(0)].index)\n",
    "\n",
    "# Display Unique Values of categorical variables\n",
    "categorical_columns = [\"Intake Type\", \"Intake Condition\", \"Outcome Type\", \"Outcome Subtype\", \"Animal Type\"]\n",
    "\n",
    "for col in categorical_columns:\n",
    "    print(f\"Unique values for {col}: {df_joined[col].unique()}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Display the number of missing values in each column\n",
    "print(df_joined.isna().sum())\n",
    "display(df_joined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7282c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "\n",
    "df_joined[\"MonthYear_intake_datetime\"] = [datetime(year=y,month=m,day=1) for [y,m] in zip(df_joined['DateTime_intake'].dt.year, df_joined['DateTime_intake'].dt.month)]\n",
    "# df_joined['MonthYear_intake_datetime'].dt.date.value_counts().sort_index().plot(kind='line',color='b')\n",
    "\n",
    "df_joined[\"MonthYear_outcome_datetime\"] = [datetime(year=y,month=m,day=1) for [y,m] in zip(df_joined['DateTime_outcome'].dt.year, df_joined['DateTime_outcome'].dt.month)]\n",
    "# df_joined['MonthYear_outcome_datetime'].dt.date.value_counts().sort_index().plot(kind='line',color='b',linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "fgh = plt.figure(figsize=(20,4))\n",
    "sns.histplot(data=df_joined, x='MonthYear_intake_datetime', hue='Animal Type', element='poly', fill=False, binwidth=30)\n",
    "sns.histplot(data=df_joined, x='MonthYear_outcome_datetime', hue='Animal Type', element='poly', fill=False, linestyle='--', binwidth=30)\n",
    "\n",
    "fgh.savefig('Intake_Outcome_AnimalType_Timeline.png')\n",
    "\n",
    "def func(pct, allvals):\n",
    "    absolute = int(np.round(pct/100.*np.sum(allvals)))\n",
    "    return f\"{pct:.1f}%\\n({absolute:d} g)\"\n",
    "\n",
    "\n",
    "fgh, axs = plt.subplots(1,2, figsize = (16,8))\n",
    "\n",
    "data   = df_joined['Outcome Type'].value_counts()\n",
    "labels = data.index.to_list()\n",
    "\n",
    "ax = axs[0]\n",
    "wedges, texts, autotexts = ax.pie(data, autopct=lambda pct: func(pct, data), textprops=dict(color=\"w\"))\n",
    "ax.legend(wedges, labels, title=\"Animal Type\", loc=\"center left\", bbox_to_anchor=(0.9, 0, 0.5, 1))\n",
    "plt.setp(autotexts, size=12, weight=\"bold\")\n",
    "ax.set_title(\"Outcome Types\", fontsize=20)\n",
    "\n",
    "df_joined['Adoption_Boolean'] = df_joined['Outcome Type'] == 'Adoption'\n",
    "data = df_joined['Adoption_Boolean'].value_counts()\n",
    "labels = ['Adoption', 'Not Adoption']\n",
    "ax = axs[1]\n",
    "wedges, texts, autotexts = ax.pie(data, autopct=lambda pct: func(pct, data), textprops=dict(color=\"w\"))\n",
    "ax.legend(wedges, labels, title=\"Animal Type\", loc=\"center left\", bbox_to_anchor=(0.9, 0, 0.5, 1))\n",
    "plt.setp(autotexts, size=12, weight=\"bold\")\n",
    "ax.set_title(\"Positive vs Negative outcome\", fontsize=20)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fgh.savefig('OutcomeType_Pie.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b302c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined['Outcome Type'].value_counts().plot(kind='bar',color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549751d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_tmp = df_joined.loc[df_joined['Intake Type'].isin(['Stray', 'Owner Surrender', 'Public Assist','Abandoned'])]\n",
    "fgh = plt.figure(figsize=(16,8))\n",
    "ax = sns.violinplot(data=df_joined_tmp.loc[df_joined['duration_in_shelter']<=timedelta(days=200)], x='Animal Type', y='duration_in_shelter', hue='Intake Type', palette='deep')\n",
    "ax.set_title('Duration In Shelter Violin Plots', fontsize=20)\n",
    "xticks = ax.get_xticklabels()\n",
    "counts = [np.sum(df_joined['Animal Type'] == xt.get_text()) for xt in xticks]\n",
    "percents = [np.mean(df_joined['Animal Type'] == xt.get_text()) for xt in xticks]\n",
    "xticks_new = []\n",
    "for xt in xticks:\n",
    "    xt.set_text(f\"{xt.get_text()}(Count={counts.pop(0)} : Pct={percents.pop(0):.2f})\")\n",
    "ax.set_xticklabels(xticks)\n",
    "fgh.savefig('DurationInShelter_IntakeType_Violin.png')\n",
    "\n",
    "df_joined_tmp = df_joined.loc[df_joined['Outcome Type'].isin(['Adoption', 'Transfer', 'Return to Owner','Euthanasia'])]\n",
    "\n",
    "fgh = plt.figure(figsize=(16,8))\n",
    "ax = sns.violinplot(data=df_joined_tmp.loc[df_joined['duration_in_shelter']<=timedelta(days=200)], x='Animal Type', y='duration_in_shelter', hue='Outcome Type', palette='deep')\n",
    "ax.set_title('Duration In Shelter Violin Plots', fontsize=20)\n",
    "xticks = ax.get_xticklabels()\n",
    "counts = [np.sum(df_joined['Animal Type'] == xt.get_text()) for xt in xticks]\n",
    "percents = [np.mean(df_joined['Animal Type'] == xt.get_text()) for xt in xticks]\n",
    "xticks_new = []\n",
    "for xt in xticks:\n",
    "    xt.set_text(f\"{xt.get_text()}(Count={counts.pop(0)} : Pct={percents.pop(0):.2f})\")\n",
    "ax.set_xticklabels(xticks)\n",
    "\n",
    "fgh.savefig('DurationInShelter_OutcomeType_Violin.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773bf4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "xticks_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_cats = df_joined.loc[df_joined['Animal Type'] == 'Cat']\n",
    "fgh = plt.figure(figsize=(20,4))\n",
    "sns.histplot(data=df_joined_cats, x='MonthYear_intake_datetime', hue='Intake Type', element='poly', fill=False, binwidth=30)\n",
    "sns.histplot(data=df_joined_cats, x='MonthYear_outcome_datetime', hue='Intake Type', element='poly', fill=False, linestyle='--', binwidth=30)\n",
    "fgh.savefig('CatIntakeTypes_histplot.png')\n",
    "fgh = plt.figure(figsize=(20,4))\n",
    "sns.histplot(data=df_joined_cats, x='MonthYear_intake_datetime', hue='Outcome Type', element='poly', fill=False, binwidth=30)\n",
    "sns.histplot(data=df_joined_cats, x='MonthYear_outcome_datetime', hue='Outcome Type', element='poly', fill=False, linestyle='--', binwidth=30)\n",
    "fgh.savefig('CatOutcomeTypes_histplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47739466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Distribution of Outcome Types\n",
    "plt.figure()\n",
    "df_joined['Outcome Type'].value_counts().plot(kind='bar')\n",
    "plt.title('Outcome Type Counts')\n",
    "\n",
    "df_joined['Adoption_Boolean'] = df_joined['Outcome Type'] == 'Adoption'\n",
    "plt.figure()\n",
    "df_joined['Adoption_Boolean'].value_counts().plot(kind='bar')\n",
    "plt.title('Adoption Successes')\n",
    "\n",
    "fgh = plt.figure(figsize=(20,4))\n",
    "df_joined[\"MonthYear_intake_datetime\"] = [datetime(year=y,month=m,day=1) for [y,m] in zip(df_joined['DateTime_intake'].dt.year, df_joined['DateTime_intake'].dt.month)]\n",
    "df_joined['MonthYear_intake_datetime'].dt.date.value_counts().sort_index().plot(kind='line',color='b')\n",
    "ax = fgh.axes\n",
    "\n",
    "df_joined[\"MonthYear_outcome_datetime\"] = [datetime(year=y,month=m,day=1) for [y,m] in zip(df_joined['DateTime_outcome'].dt.year, df_joined['DateTime_outcome'].dt.month)]\n",
    "df_joined['MonthYear_outcome_datetime'].dt.date.value_counts().sort_index().plot(kind='line',color='b',linestyle='--')\n",
    "\n",
    "\n",
    "animal_types = df_joined['Animal Type'].unique()\n",
    "linecolors = 'rgcmk'\n",
    "for at, clr in zip(animal_types, linecolors):\n",
    "    df_joined['MonthYear_intake_datetime'].loc[df_joined['Animal Type'] == at].dt.date.value_counts().sort_index().plot(kind='line',color=clr)\n",
    "    df_joined['MonthYear_outcome_datetime'].loc[df_joined['Animal Type'] == at].dt.date.value_counts().sort_index().plot(kind='line',color=clr,linestyle='--')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9c6f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at cat seasonality\n",
    "fgh = plt.figure(figsize=(20,4))\n",
    "ax = fgh.axes\n",
    "df_joined_cats = df_joined.loc[df_joined['Animal Type']=='Dog']\n",
    "\n",
    "outtypes = df_joined_cats['Outcome Type'].unique()\n",
    "linecolors = 'rgcmkyb'\n",
    "for outcome_type, clr in zip(outtypes,linecolors):\n",
    "    df_joined_cats['MonthYear_intake_datetime'].loc[df_joined_cats['Outcome Type'] == outcome_type].value_counts().sort_index().plot(kind='line',color=clr,='hi')\n",
    "    df_joined_cats['MonthYear_outcome_datetime'].loc[df_joined_cats['Outcome Type'] == outcome_type].value_counts().sort_index().plot(kind='line',color=clr,linestyle='--')\n",
    "\n",
    "print(outtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5271e",
   "metadata": {},
   "source": [
    "## Sex and Fixed status as well as age upon intake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgh,axes = plt.subplots(2,1)\n",
    "axes = axes.flatten()\n",
    "df_joineda = df_joined.loc[df_joined['Outcome Type'] == 'Adoption']\n",
    "axes[0].hist(df_joineda['duration_in_shelter'].dt.days,bins=200)\n",
    "axes[1].hist(df_joineda['duration_in_shelter'].loc[df_joineda['duration_in_shelter'].dt.days<=50].dt.days,bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688347bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the month and year from the intake date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e91d8c",
   "metadata": {},
   "source": [
    "## Color in RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Colors\n",
    "def process_color(color):\n",
    "    color = color.lower()\n",
    "    color = color.replace('/', ' ') \n",
    "    color = color.split(' ')\n",
    "    return(color)\n",
    "\n",
    "def color_dict():\n",
    "    color_dict = {\n",
    "        'buff': [218, 160, 109], \n",
    "        'white': [255, 255, 255], \n",
    "        'fawn': [196, 164, 132], \n",
    "        'apricot':[251, 206, 177], \n",
    "        'black':[0,0,0],\n",
    "        'silver':[192, 192, 192],\n",
    "        'brindle':[130,119,107], \n",
    "        'pink':[255, 192, 203], \n",
    "        'torbie':[255,184,90], \n",
    "        'lynx':[162,113,80], \n",
    "        'sable':[110, 64, 60], \n",
    "        'gray':[128,128,128],\n",
    "        'blue':[0, 0, 255],\n",
    "        'calico':[210, 170, 133],\n",
    "        'red':[255,0,0],\n",
    "        'flame':[226, 88, 34],\n",
    "        'liver':[83, 75, 79],\n",
    "        'lilac':[200, 162, 200],\n",
    "        'chocolate':[123, 63, 0],\n",
    "        'yellow':[255, 255, 0],\n",
    "        'cream':[255, 253, 208],\n",
    "        'orange':[255, 165, 0],\n",
    "        'tiger':[203,113,25],\n",
    "        'gold':[255,215,0],\n",
    "        'tan':[210, 180, 140],\n",
    "        'brown':[165, 42, 42],\n",
    "        'ruddy':[255, 0, 40],                \n",
    "        'seal':[50, 20, 20],        \n",
    "        'green':[0, 255, 0],\n",
    "        'smoke':[132, 136, 132]\n",
    "    }\n",
    "    return(color_dict)\n",
    "    \n",
    "def map_colors(colors, color_dict):\n",
    "    rgb_triplets = []\n",
    "    for color in colors:\n",
    "        if color in color_dict.keys():\n",
    "            rgb_triplets.append(color_dict[color])\n",
    "\n",
    "    if rgb_triplets == []:\n",
    "        return([np.nan, np.nan, np.nan])\n",
    "    else:\n",
    "        return(np.mean(rgb_triplets, axis=0))\n",
    "            \n",
    "    \n",
    "df_joined['Color_processed'] = df_joined['Color'].apply(process_color)\n",
    "df_joined['Colors_RGB'] = df_joined['Color_processed'].apply(map_colors, color_dict=color_dict())\n",
    "\n",
    "df_joined = pd.concat([df_joined, pd.DataFrame(df_joined['Colors_RGB'].to_list(), columns=['Color_R', 'Color_G', 'Color_B'],index=df_joined.index)], axis=1)\n",
    "df_joined = df_joined.drop(['Color_processed', 'Colors_RGB'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36eee21",
   "metadata": {},
   "source": [
    "## Location found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef74f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def load_bert_models(LOAD_FROM_FILES=True):\n",
    "    if LOAD_FROM_FILES:\n",
    "        with open('bert_tokenizer.pkl', 'rb') as f:\n",
    "            bert_tokenizer = pickle.load(f)\n",
    "        with open('bert_model.pkl', 'rb') as f:\n",
    "            bert_model = pickle.load(f)\n",
    "    else:\n",
    "        bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        bert_model     = BertModel.from_pretrained('bert-base-uncased')    \n",
    "        with open('bert_tokenizer.pkl', 'wb') as f:\n",
    "            pickle.dump(bert_tokenizer, f)\n",
    "\n",
    "        with open('data','bert_model.pkl', 'wb') as f:\n",
    "            pickle.dump(bert_model, f)\n",
    "\n",
    "    return(bert_tokenizer, bert_model)\n",
    "\n",
    "def get_bert_embeddings(bert_tokenizer, bert_model, text, batch_size=10000):\n",
    "\n",
    "    embeds_list = []\n",
    "    time_list = []\n",
    "    for i in range(0, len(text), batch_size):\n",
    "        start = time.time()\n",
    "        text_batch = text[i:i+batch_size]\n",
    "        bert_inputs = bert_tokenizer(text_batch, return_tensors='pt',padding=True)\n",
    "\n",
    "        # Calculate the bert outputs\n",
    "        with torch.no_grad():\n",
    "            bert_outputs = bert_model(**bert_inputs)\n",
    "\n",
    "        # Final embedding vector is the average of all token vectors    \n",
    "        avg_hidden_state = bert_outputs.last_hidden_state.mean(dim=1).squeeze(1)\n",
    "        \n",
    "        embeds_list.append(avg_hidden_state)\n",
    "        end = time.time()\n",
    "        time_list.append(end-start)\n",
    "\n",
    "        embeds_size = np.sum([np.prod(x.shape) for x in embeds_list])*4/(1024**3)\n",
    "        print(f\"Processed {i + len(text_batch)}/{len(text)} texts. Elapsed Time = {np.sum(time_list)}s. Avg Time = {np.mean(time_list)}. Embeddings Size = {embeds_size} GB\",end='\\r')\n",
    "\n",
    "    embeds_tensor = torch.vstack(embeds_list)\n",
    "    return(embeds_tensor)\n",
    "\n",
    "def pca_reduction(embeddings,var_threshold=0.90):\n",
    "    sc = StandardScaler()\n",
    "    embeddings = sc.fit_transform(embeddings)\n",
    "\n",
    "    pca = PCA()\n",
    "    embeddings_pca = pca.fit_transform(embeddings)\n",
    "    embeddings_pca = embeddings_pca[:,np.cumsum(pca.explained_variance_ratio_) < var_threshold]\n",
    "    return(embeddings_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa9563",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer, bert_model = load_bert_models(LOAD_FROM_FILES=True)\n",
    "# df_processed = pd.read_pickle(os.path.join('data', f'Austin_Animal_Center_Joined_{download_datestr}_processed.pkl'))\n",
    "\n",
    "# Process Breed\n",
    "df_joined['Breed_mixed'] = df_joined['Breed'].str.endswith(' Mix')\n",
    "df_joined['Breed_processed'] = df_joined['Breed'].str.replace(' Mix', '')\n",
    "df_joined['Breed_processed'] = df_joined['Breed_processed'].str.lower()\n",
    "breed_list = df_joined['Breed_processed'].to_list()\n",
    "breed_embeddings = get_bert_embeddings(bert_tokenizer, bert_model, breed_list,batch_size=1000) \n",
    "# breed_embeddings = pca_reduction(breed_embeddings, var_threshold=0.9)\n",
    "\n",
    "df_joined['Name_processed'] = df_joined['Name'].str.lower()\n",
    "df_joined['Name_processed'] = df_joined['Name_processed'].str.replace('*', '')\n",
    "name_list = df_joined['Name_processed'].to_list()\n",
    "name_embeddings = get_bert_embeddings(bert_tokenizer, bert_model, name_list, batch_size=1000) \n",
    "# name_embeddings = pca_reduction(name_embeddings, var_threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a0f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert name_embeddings and breed_embeddings to pandas dataframes with same index as df_joined\n",
    "name_embeddings = pd.DataFrame(name_embeddings.numpy(), index=df_joined.index, columns=[f'name_embedding_{i}' for i in range(name_embeddings.shape[1])])\n",
    "breed_embeddings = pd.DataFrame(breed_embeddings.numpy(), index=df_joined.index, columns=[f'breed_embedding_{i}' for i in range(breed_embeddings.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eceaadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_joined.join(name_embeddings)\n",
    "df_joined = df_joined.join(breed_embeddings)\n",
    "\n",
    "#save to pickle\n",
    "df_joined.to_pickle(os.path.join('data', f'Austin_Animal_Center_Joined_{download_date}_processed.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404b3d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot variance explained by PCA components\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ndim_bert = 768\n",
    "name_embedding_cols = [f'name_embedding_{i}' for i in range(ndim_bert)]\n",
    "breed_embedding_cols = [f'breed_embedding_{i}' for i in range(ndim_bert)]\n",
    "df_processed = pd.read_pickle(os.path.join('data', f'Austin_Animal_Center_Joined_{download_datestr}_processed.pkl'))\n",
    "sc = StandardScaler()\n",
    "\n",
    "embeddings_name = df_processed[name_embedding_cols].values\n",
    "embeddings_breed = df_processed[breed_embedding_cols].values\n",
    "embeddings_name_norm = sc.fit_transform(embeddings_name)\n",
    "embeddings_breed_norm = sc.fit_transform(embeddings_breed)\n",
    "\n",
    "pca_name = PCA()\n",
    "pca_breed = PCA()\n",
    "embeddings_name_pca = pca_name.fit_transform(embeddings_name_norm)\n",
    "embeddings_breed_pca = pca_breed.fit_transform(embeddings_breed_norm)\n",
    "\n",
    "# embeddings_pca = embeddings_pca[:,np.cumsum(pca.explained_variance_ratio_) < var_threshold]\n",
    "\n",
    "# plot variance explained by PCA components\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca_name.explained_variance_ratio_))\n",
    "plt.plot(np.cumsum(pca_breed.explained_variance_ratio_))\n",
    "\n",
    "# Plot horizontal dashed lines at a y value of 0.9 and 0.95\n",
    "plt.axhline(y=0.9, color='gray', linestyle='--')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.legend(['Name','Breed','90% Explained','95% Explained'])\n",
    "plt.savefig('PCA_variance_explained_name_bre')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e198b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from geopy.geocoders import Nominatim\n",
    "\n",
    "# geolocator = Nominatim(user_agent=\"csmlFinalProject\")\n",
    "\n",
    "# # Get the Austin Animal Shelter Location\n",
    "# getLoc = geolocator.geocode(\"Austin, Texas\")\n",
    "\n",
    "# # printing address\n",
    "# print(getLoc.address)\n",
    "\n",
    "# # printing latitude and longitude\n",
    "# print(\"Latitude = \", getLoc.latitude)\n",
    "# print(\"Longitude = \", getLoc.longitude)\n",
    "\n",
    "# for addy in df_joined['Found Location'].iloc[0:50]:\n",
    "#     print(addy)\n",
    "#     # getLoc = geolocator.geocode(addy)\n",
    "#     # print(getLoc.address)\n",
    "\n",
    "# address_list = []\n",
    "# for addy in df_joined['Found Location'].str.split(' '):    \n",
    "#     city_str = addy[-2]\n",
    "#     state_str = addy[-1][1:3]\n",
    "#     addr_str = addy[0:-3]\n",
    "    \n",
    "#     # First try the address type of string\n",
    "\n",
    "#     loc = geolocator.geocode()\n",
    "\n",
    "# street, city, county, state, country, or postalcode\n",
    "\n",
    "# df_joined['city_found'] = city_str\n",
    "# print(df_joined['city_found'].unique())\n",
    "\n",
    "\n",
    "# df_joined['state_found'] = state_str\n",
    "# df_joined['state_found'] = df_joined['state_found'].replace('ur', 'Outside State')\n",
    "# print(df_joined['state_found'].unique())\n",
    "\n",
    "# for addy in addys:\n",
    "#     for word in addy:        \n",
    "#         if word.lower() == 'and':\n",
    "#             print(addy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdecd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Cross validation on Adoption classification performance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Drop columns that are not needed for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e5c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "intake_month_counts = df_joined['intake_month'].value_counts(sort=False)\n",
    "print(intake_month_counts.sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306d93be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "ndim_bert = 768\n",
    "name_embedding_cols = [f'name_embedding_{i}' for i in range(ndim_bert)]\n",
    "breed_embedding_cols = [f'breed_embedding_{i}' for i in range(ndim_bert)]\n",
    "standard_features = ['Intake Type', 'Intake Condition', 'Sex upon Intake', 'Age upon Intake',\n",
    "       'DateTime_outcome', 'Date of Birth', 'Outcome Type', 'Outcome Subtype',\n",
    "       'Sex upon Outcome', 'Age upon Outcome', 'Name', 'Animal Type', 'Breed',\n",
    "       'Color', 'duration_in_shelter', 'age_upon_intake_years', 'fixed', 'sex',\n",
    "       'Color_R', 'Color_G', 'Color_B', 'city_found', 'state_found',\n",
    "       'Breed_mixed']\n",
    "\n",
    "categorical_features = []\n",
    "# my_features = \n",
    "\n",
    "# Different Pipelines\n",
    "# 1) Basic Pipeline with all categorical features\n",
    "# 2) Basic Pipeline with one-hot-encoded categorical features\n",
    "# 3) Pipeline with categorical and embedded/engineered features\n",
    "# 4) Pipeline with one-hot-encoded and embedded/engineered features\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('ct_ss', \n",
    "            ColumnTransformer(\n",
    "             [(\"ss_name\", StandardScaler(), name_embedding_cols),\n",
    "              (\"ss_breed\", StandardScaler(), breed_embedding_cols),\n",
    "             (\"pass\", \"passthrough\", standard_features)])\n",
    "        ),\n",
    "        ('ct_pca', \n",
    "            ColumnTransformer(\n",
    "             [(\"PCA_name\", PCA(), name_embedding_cols),\n",
    "              (\"PCA_breed\", PCA(), breed_embedding_cols),\n",
    "             (\"pass\", \"passthrough\", standard_features)])\n",
    "        ),\n",
    "         ('svc', SVC())\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_embeds = Pipeline([(\"scaler\",StandardScaler()),(\"pca\",PCA())])\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('ct', \n",
    "            ColumnTransformer(\n",
    "             [(\"name_feats\", pipe_embeds, name_embedding_cols),\n",
    "              (\"breed_feats\", pipe_embeds, breed_embedding_cols),\n",
    "             (\"pass\", \"passthrough\", standard_features)])\n",
    "        ),\n",
    "         ('svc', SVC())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289aa7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b955a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3952d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 30\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# Set up possible values of parameters to optimize over\n",
    "p_grid = {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}\n",
    "\n",
    "# We will use a Support Vector Classifier with \"rbf\" kernel\n",
    "svm = SVC(kernel=\"rbf\")\n",
    "\n",
    "# Arrays to store scores\n",
    "non_nested_scores = np.zeros(NUM_TRIALS)\n",
    "nested_scores = np.zeros(NUM_TRIALS)\n",
    "\n",
    "# Loop for each trial\n",
    "for i in range(NUM_TRIALS):\n",
    "    # Choose cross-validation techniques for the inner and outer loops,\n",
    "    # independently of the dataset.\n",
    "    # E.g \"GroupKFold\", \"LeaveOneOut\", \"LeaveOneGroupOut\", etc.\n",
    "    inner_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n",
    "    outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n",
    "\n",
    "    # Non_nested parameter search and scoring\n",
    "    clf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=outer_cv)\n",
    "    clf.fit(X_iris, y_iris)\n",
    "    non_nested_scores[i] = clf.best_score_\n",
    "\n",
    "    # Nested CV with parameter optimization\n",
    "    clf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=inner_cv)\n",
    "    nested_score = cross_val_score(clf, X=X_iris, y=y_iris, cv=outer_cv)\n",
    "    nested_scores[i] = nested_score.mean()\n",
    "\n",
    "score_difference = non_nested_scores - nested_scores\n",
    "\n",
    "print(\n",
    "    \"Average difference of {:6f} with std. dev. of {:6f}.\".format(\n",
    "        score_difference.mean(), score_difference.std()\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot scores on each trial for nested and non-nested CV\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "(non_nested_scores_line,) = plt.plot(non_nested_scores, color=\"r\")\n",
    "(nested_line,) = plt.plot(nested_scores, color=\"b\")\n",
    "plt.ylabel(\"score\", fontsize=\"14\")\n",
    "plt.legend(\n",
    "    [non_nested_scores_line, nested_line],\n",
    "    [\"Non-Nested CV\", \"Nested CV\"],\n",
    "    bbox_to_anchor=(0, 0.4, 0.5, 0),\n",
    ")\n",
    "plt.title(\n",
    "    \"Non-Nested and Nested Cross Validation on Iris Dataset\",\n",
    "    x=0.5,\n",
    "    y=1.1,\n",
    "    fontsize=\"15\",\n",
    ")\n",
    "\n",
    "# Plot bar chart of the difference.\n",
    "plt.subplot(212)\n",
    "difference_plot = plt.bar(range(NUM_TRIALS), score_difference)\n",
    "plt.xlabel(\"Individual Trial #\")\n",
    "plt.legend(\n",
    "    [difference_plot],\n",
    "    [\"Non-Nested CV - Nested CV Score\"],\n",
    "    bbox_to_anchor=(0, 1, 0.8, 0),\n",
    ")\n",
    "plt.ylabel(\"score difference\", fontsize=\"14\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
